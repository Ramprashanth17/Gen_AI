{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ PromptCraft: AI Prompt Analyzer & Optimizer\n",
    "\n",
    "## A Crash Course in Generative AI: Understanding and Engineering Effective Prompts\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** November 2024  \n",
    "**Course:** INFO 7390 - Art and Science of Data  \n",
    "**Topic:** Controllable Text Generation Techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Abstract\n",
    "\n",
    "As Large Language Models (LLMs) become integral to workflows across industries, the quality of human-AI interaction increasingly depends on **prompt engineering** â€“ the art and science of crafting effective instructions for AI systems.\n",
    "\n",
    "This notebook explores:\n",
    "\n",
    "1. **Theoretical Foundations**: Established prompt engineering frameworks (CO-STAR, CRISPE, RISEN, RACE) and the principles that make prompts effective\n",
    "\n",
    "2. **Technical Deep-Dive**: How LLMs process prompts through tokenization, attention mechanisms, and context windows\n",
    "\n",
    "3. **Practical Implementation**: Building a prompt quality scoring system that evaluates and optimizes user prompts\n",
    "\n",
    "4. **Multi-Model Analysis**: Comparing how different LLMs (Claude, Gemini) respond to prompt optimization\n",
    "\n",
    "By the end of this notebook, you will understand not just *how* to write better prompts, but *why* certain techniques work at a fundamental level â€“ enabling you to systematically improve AI interactions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Table of Contents\n",
    "\n",
    "1. [Introduction: Why Prompt Engineering Matters](#1-introduction)\n",
    "2. [Theoretical Foundations](#2-theory)\n",
    "   - 2.1 [What Makes a Prompt \"Good\"?](#2.1-good-prompts)\n",
    "   - 2.2 [Established Frameworks](#2.2-frameworks)\n",
    "   - 2.3 [Universal Prompt Dimensions](#2.3-dimensions)\n",
    "3. [Technical Deep-Dive: How LLMs Process Prompts](#3-technical)\n",
    "   - 3.1 [Tokenization](#3.1-tokenization)\n",
    "   - 3.2 [Attention Mechanisms](#3.2-attention)\n",
    "   - 3.3 [Context Windows and Prompt Structure](#3.3-context)\n",
    "4. [Building the Prompt Scorer](#4-implementation)\n",
    "   - 4.1 [Scoring Methodology](#4.1-methodology)\n",
    "   - 4.2 [Rule-Based Analysis](#4.2-rule-based)\n",
    "   - 4.3 [LLM-Based Quality Assessment](#4.3-llm-based)\n",
    "   - 4.4 [Combined Scoring System](#4.4-combined)\n",
    "5. [Prompt Optimization Engine](#5-optimization)\n",
    "6. [Multi-Model Comparison](#6-comparison)\n",
    "7. [Conclusion](#7-conclusion)\n",
    "8. [References](#8-references)\n",
    "9. [License](#9-license)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-introduction'></a>\n",
    "## 1. ğŸŒŸ Introduction: Why Prompt Engineering Matters\n",
    "\n",
    "### The $200 Billion Question\n",
    "\n",
    "The generative AI market is projected to reach $200+ billion by 2030. Yet studies show that **most users fail to effectively communicate with AI systems**, leading to:\n",
    "\n",
    "- Frustrating interactions and abandoned tasks\n",
    "- Suboptimal outputs requiring multiple iterations\n",
    "- Underutilization of AI capabilities\n",
    "- Hallucinations triggered by ambiguous instructions\n",
    "\n",
    "### The Prompt Engineering Gap\n",
    "\n",
    "Consider these two prompts for the same task:\n",
    "\n",
    "**Prompt A (Naive):**\n",
    "```\n",
    "Write about climate change\n",
    "```\n",
    "\n",
    "**Prompt B (Engineered):**\n",
    "```\n",
    "Role: You are an environmental science educator.\n",
    "\n",
    "Context: I'm creating content for high school students (ages 15-17) \n",
    "who have basic science knowledge but limited exposure to climate topics.\n",
    "\n",
    "Task: Explain the greenhouse effect and its role in climate change.\n",
    "\n",
    "Requirements:\n",
    "- Use 2-3 relatable analogies (e.g., car on a hot day)\n",
    "- Include one simple diagram description\n",
    "- Length: 300-400 words\n",
    "- Tone: Engaging but scientifically accurate\n",
    "\n",
    "Avoid: Political arguments, doom-and-gloom framing, jargon without explanation\n",
    "```\n",
    "\n",
    "The difference in output quality is **dramatic** â€“ yet most users default to Prompt A.\n",
    "\n",
    "### Our Goal\n",
    "\n",
    "Build a system that:\n",
    "1. **Analyzes** any prompt and identifies weaknesses\n",
    "2. **Scores** the prompt on established quality dimensions\n",
    "3. **Optimizes** the prompt using LLM assistance\n",
    "4. **Educates** users on *why* the improvements work\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-theory'></a>\n",
    "## 2. ğŸ“– Theoretical Foundations\n",
    "\n",
    "<a id='2.1-good-prompts'></a>\n",
    "### 2.1 What Makes a Prompt \"Good\"?\n",
    "\n",
    "Research and industry practice have identified several characteristics of effective prompts:\n",
    "\n",
    "| Characteristic | Description | Impact |\n",
    "|----------------|-------------|--------|\n",
    "| **Specificity** | Clear, unambiguous instructions | Reduces hallucination, improves relevance |\n",
    "| **Context Richness** | Background information and constraints | Enables appropriate responses |\n",
    "| **Role Definition** | Assigning expertise/persona to the AI | Activates relevant knowledge patterns |\n",
    "| **Output Specification** | Format, length, style requirements | Ensures usable outputs |\n",
    "| **Constraint Setting** | Boundaries and exclusions | Prevents unwanted content |\n",
    "| **Structure** | Organized, parseable format | Improves comprehension |\n",
    "\n",
    "#### Why These Work: The Cognitive Science Perspective\n",
    "\n",
    "LLMs are trained on human text, which means they respond to the same structural cues that help humans understand tasks:\n",
    "\n",
    "- **Headers and sections** â†’ Signal topic transitions\n",
    "- **Bullet points** â†’ Indicate discrete requirements\n",
    "- **Examples** â†’ Demonstrate expected patterns\n",
    "- **Explicit constraints** â†’ Define decision boundaries\n",
    "\n",
    "This isn't anthropomorphization â€“ it's recognizing that the training data encodes human communication patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.2-frameworks'></a>\n",
    "### 2.2 Established Prompt Engineering Frameworks\n",
    "\n",
    "Several frameworks have emerged to systematize prompt engineering:\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸŒŸ CO-STAR Framework\n",
    "*Developed by Singapore's GovTech for government AI applications*\n",
    "\n",
    "| Letter | Element | Purpose |\n",
    "|--------|---------|--------|\n",
    "| **C** | Context | Background information for the task |\n",
    "| **O** | Objective | The specific goal to achieve |\n",
    "| **S** | Style | Writing style (formal, casual, technical) |\n",
    "| **T** | Tone | Emotional quality (professional, friendly, urgent) |\n",
    "| **A** | Audience | Who will consume the output |\n",
    "| **R** | Response | Format and structure requirements |\n",
    "\n",
    "**Best for:** Content creation, communication tasks, customer-facing outputs\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”§ CRISPE Framework\n",
    "*Popular in technical and development contexts*\n",
    "\n",
    "| Letter | Element | Purpose |\n",
    "|--------|---------|--------|\n",
    "| **C** | Capacity/Role | The expertise the AI should assume |\n",
    "| **R** | Insight | Context and background information |\n",
    "| **I** | Statement | The specific task or question |\n",
    "| **S** | Personality | Voice and style characteristics |\n",
    "| **P** | Experiment | Request for variations or alternatives |\n",
    "| **E** | Exclusions | What to avoid (added by practitioners) |\n",
    "\n",
    "**Best for:** Technical writing, code generation, analytical tasks\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“ˆ RISEN Framework\n",
    "*Emphasizes structured task breakdown*\n",
    "\n",
    "| Letter | Element | Purpose |\n",
    "|--------|---------|--------|\n",
    "| **R** | Role | Define AI's expertise area |\n",
    "| **I** | Instructions | Detailed task description |\n",
    "| **S** | Steps | Break complex tasks into stages |\n",
    "| **E** | End Goal | Clear success criteria |\n",
    "| **N** | Narrowing | Constraints and scope limits |\n",
    "\n",
    "**Best for:** Complex multi-step tasks, process documentation, planning\n",
    "\n",
    "---\n",
    "\n",
    "#### âš¡ RACE Framework\n",
    "*Minimalist approach for quick prompts*\n",
    "\n",
    "| Letter | Element | Purpose |\n",
    "|--------|---------|--------|\n",
    "| **R** | Role | Who is the AI? |\n",
    "| **A** | Action | What should it do? |\n",
    "| **C** | Context | Background information |\n",
    "| **E** | Expectation | Desired output format |\n",
    "\n",
    "**Best for:** Quick queries, simple tasks, iterative conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2.3-dimensions'></a>\n",
    "### 2.3 Universal Prompt Dimensions\n",
    "\n",
    "Analyzing these frameworks reveals **six universal dimensions** that appear across all of them:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  UNIVERSAL PROMPT DIMENSIONS                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   1. ROLE/PERSONA        What expertise should the AI assume?   â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   2. CONTEXT             What background is needed?              â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   3. TASK/INSTRUCTION    What specifically needs to be done?    â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   4. OUTPUT FORMAT       How should results be structured?      â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   5. CONSTRAINTS         What are the boundaries/exclusions?    â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   6. STYLE/TONE          What voice and feel is desired?        â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**These six dimensions form the basis of our scoring system.**\n",
    "\n",
    "#### Framework Comparison Matrix\n",
    "\n",
    "| Dimension | CO-STAR | CRISPE | RISEN | RACE |\n",
    "|-----------|---------|--------|-------|------|\n",
    "| Role | â—‹ | C (Capacity) | R | R |\n",
    "| Context | C | R (Insight) | - | C |\n",
    "| Task | O | I (Statement) | I | A |\n",
    "| Format | R | - | E | E |\n",
    "| Constraints | - | E (Exclusions) | N | - |\n",
    "| Style/Tone | S, T | S (Personality) | - | - |\n",
    "| Audience | A | - | - | - |\n",
    "| Steps | - | - | S | - |\n",
    "| Variations | - | P (Experiment) | - | - |\n",
    "\n",
    "This analysis shows that **no single framework is complete** â€“ but combining their insights gives us a comprehensive evaluation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='3-technical'></a>\n",
    "## 3. ğŸ”¬ Technical Deep-Dive: How LLMs Process Prompts\n",
    "\n",
    "Understanding *why* prompt engineering works requires looking under the hood at how LLMs process text.\n",
    "\n",
    "<a id='3.1-tokenization'></a>\n",
    "### 3.1 Tokenization: The First Step\n",
    "\n",
    "LLMs don't see words â€“ they see **tokens**. Understanding tokenization helps explain:\n",
    "- Why some prompts cost more (API pricing)\n",
    "- Why certain phrasings work better\n",
    "- How context windows limit prompt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install the required packages\n",
    "!pip install tiktoken transformers matplotlib plotly pandas numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the tokenizer (GPT-4/Claude use similar BPE tokenization)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")  # Used by GPT-4, similar to Claude\n",
    "\n",
    "def visualize_tokens(text, title=\"Tokenization Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualize how a text string is broken into tokens.\n",
    "    \n",
    "    This helps understand:\n",
    "    - Token boundaries don't always match word boundaries\n",
    "    - Common words = fewer tokens\n",
    "    - Technical terms may split unexpectedly\n",
    "    \"\"\"\n",
    "    tokens = encoding.encode(text)\n",
    "    token_strings = [encoding.decode([t]) for t in tokens]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nOriginal text ({len(text)} characters):\")\n",
    "    print(f'\"{text}\"')\n",
    "    print(f\"\\nToken count: {len(tokens)}\")\n",
    "    print(f\"\\nTokens breakdown:\")\n",
    "    \n",
    "    # Color-code tokens for visualization\n",
    "    colors = ['\\033[91m', '\\033[92m', '\\033[93m', '\\033[94m', '\\033[95m', '\\033[96m']\n",
    "    reset = '\\033[0m'\n",
    "    \n",
    "    colored_output = \"\"\n",
    "    for i, token_str in enumerate(token_strings):\n",
    "        color = colors[i % len(colors)]\n",
    "        # Show token with visible boundaries\n",
    "        display_str = token_str.replace(' ', 'â£').replace('\\n', 'â†µ')\n",
    "        colored_output += f\"{color}[{display_str}]{reset}\"\n",
    "    \n",
    "    print(colored_output)\n",
    "    print(f\"\\nToken IDs: {tokens[:10]}...\" if len(tokens) > 10 else f\"\\nToken IDs: {tokens}\")\n",
    "    \n",
    "    return len(tokens), token_strings\n",
    "\n",
    "# Example: Compare tokenization of good vs bad prompts\n",
    "bad_prompt = \"Write about climate change\"\n",
    "good_prompt = \"\"\"Role: Environmental science educator\n",
    "Task: Explain greenhouse effect for high school students\n",
    "Format: 300 words with analogies\"\"\"\n",
    "\n",
    "bad_tokens, _ = visualize_tokens(bad_prompt, \"BAD PROMPT Tokenization\")\n",
    "good_tokens, _ = visualize_tokens(good_prompt, \"GOOD PROMPT Tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze the token efficiency\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOKEN EFFICIENCY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBad prompt:  {bad_tokens} tokens for {len(bad_prompt)} chars\")\n",
    "print(f\"Good prompt: {good_tokens} tokens for {len(good_prompt)} chars\")\n",
    "print(f\"\\nToken increase: {good_tokens - bad_tokens} tokens ({((good_tokens/bad_tokens)-1)*100:.1f}% more)\")\n",
    "print(f\"Character increase: {len(good_prompt) - len(bad_prompt)} chars ({((len(good_prompt)/len(bad_prompt))-1)*100:.1f}% more)\")\n",
    "print(f\"\\nğŸ’¡ Insight: A {((good_tokens/bad_tokens)-1)*100:.0f}% increase in tokens can yield\")\n",
    "print(f\"   a dramatically better response â€“ often 10x improvement in quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Tokenization Insights for Prompt Engineering\n",
    "\n",
    "1. **Structure is cheap**: Headers, bullet points, and formatting add minimal tokens but greatly improve parsing\n",
    "\n",
    "2. **Common words tokenize efficiently**: \"the\", \"and\", \"you\" are single tokens\n",
    "\n",
    "3. **Technical terms may split**: \"tokenization\" â†’ \"token\" + \"ization\" (2 tokens)\n",
    "\n",
    "4. **Whitespace matters**: Leading spaces often become separate tokens\n",
    "\n",
    "**Practical implication**: Don't sacrifice clarity to save tokens. A well-structured prompt with 100 tokens will outperform a vague prompt with 20 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.2-attention'></a>\n",
    "### 3.2 Attention Mechanisms: Why Structure Matters\n",
    "\n",
    "The **attention mechanism** is how LLMs decide which parts of the prompt are relevant to each word they generate.\n",
    "\n",
    "```\n",
    "Simplified Attention Visualization:\n",
    "\n",
    "Prompt: \"You are a Python expert. Write a function to sort a list.\"\n",
    "\n",
    "When generating \"def\":\n",
    "   You  are  a  Python  expert  Write  function  sort  list\n",
    "   [.1] [.1] [.] [.3]   [.2]   [.05]  [.15]    [.05] [.05]\n",
    "                 â†‘       â†‘            â†‘\n",
    "            High attention to relevant context\n",
    "```\n",
    "\n",
    "#### Why This Matters for Prompt Engineering\n",
    "\n",
    "1. **Position effects**: Information at the start and end of prompts often gets more attention\n",
    "2. **Explicit markers help**: \"Role:\", \"Task:\", \"Format:\" create attention anchors\n",
    "3. **Proximity matters**: Keep related information together\n",
    "4. **Repetition can help**: Restating key requirements at the end can reinforce them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate attention patterns with a simple visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def visualize_attention_concept():\n",
    "    \"\"\"\n",
    "    Conceptual visualization of attention patterns.\n",
    "    (Note: This is illustrative, not actual model attention)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simulated attention weights for demonstration\n",
    "    prompt_tokens = [\"Role:\", \"Python\", \"expert\", \"Task:\", \"Write\", \"sort\", \"function\", \"Format:\", \"Clean\", \"code\"]\n",
    "    \n",
    "    # Attention when generating different outputs\n",
    "    attention_patterns = {\n",
    "        \"Generating 'def'\": [0.15, 0.25, 0.15, 0.1, 0.1, 0.08, 0.12, 0.02, 0.02, 0.01],\n",
    "        \"Generating 'sorted'\": [0.02, 0.1, 0.05, 0.08, 0.05, 0.35, 0.25, 0.03, 0.04, 0.03],\n",
    "        \"Adding comments\": [0.05, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.2, 0.2, 0.15],\n",
    "    }\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for output, weights in attention_patterns.items():\n",
    "        fig.add_trace(go.Bar(\n",
    "            name=output,\n",
    "            x=prompt_tokens,\n",
    "            y=weights,\n",
    "            text=[f\"{w:.0%}\" for w in weights],\n",
    "            textposition='auto',\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Conceptual Attention Patterns: How LLMs Focus on Different Prompt Parts\",\n",
    "        xaxis_title=\"Prompt Tokens\",\n",
    "        yaxis_title=\"Attention Weight\",\n",
    "        barmode='group',\n",
    "        height=400,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Key Insight: Explicit markers (Role:, Task:, Format:) create 'attention anchors'\")\n",
    "    print(\"   that help the model understand prompt structure.\")\n",
    "\n",
    "visualize_attention_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3-context'></a>\n",
    "### 3.3 Context Windows and Prompt Structure\n",
    "\n",
    "Every LLM has a **context window** â€“ the maximum tokens it can process at once:\n",
    "\n",
    "| Model | Context Window | Approximate Words |\n",
    "|-------|----------------|-------------------|\n",
    "| GPT-3.5 | 4,096 tokens | ~3,000 words |\n",
    "| GPT-4 | 8,192 / 128K tokens | ~6,000 / 96,000 words |\n",
    "| Claude 3 | 200K tokens | ~150,000 words |\n",
    "| Gemini Pro | 32K / 1M tokens | ~24,000 / 750,000 words |\n",
    "\n",
    "#### The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research shows LLMs pay less attention to information in the middle of long contexts:\n",
    "\n",
    "```\n",
    "Attention Distribution in Long Contexts:\n",
    "\n",
    "    â–²\n",
    "    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ                              â–ˆâ–ˆâ–ˆâ–ˆ\n",
    "    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ                      â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ\n",
    "    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ          â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ\n",
    "    â”‚   â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º\n",
    "        Start            Middle              End\n",
    "```\n",
    "\n",
    "**Practical Implication**: Put the most important instructions at the **beginning** and **end** of your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the context window concept\n",
    "def analyze_prompt_structure(prompt):\n",
    "    \"\"\"\n",
    "    Analyze how a prompt uses its \"real estate\" within a context window.\n",
    "    \"\"\"\n",
    "    tokens = encoding.encode(prompt)\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    # Divide into sections (start, middle, end)\n",
    "    third = total_tokens // 3\n",
    "    \n",
    "    start_section = encoding.decode(tokens[:third])\n",
    "    middle_section = encoding.decode(tokens[third:2*third])\n",
    "    end_section = encoding.decode(tokens[2*third:])\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PROMPT STRUCTURE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nTotal tokens: {total_tokens}\")\n",
    "    print(f\"\\nğŸ“ START SECTION (High attention zone):\")\n",
    "    print(f\"   '{start_section[:100]}...'\" if len(start_section) > 100 else f\"   '{start_section}'\")\n",
    "    print(f\"\\nğŸ“ MIDDLE SECTION (Lower attention zone):\")\n",
    "    print(f\"   '{middle_section[:100]}...'\" if len(middle_section) > 100 else f\"   '{middle_section}'\")\n",
    "    print(f\"\\nğŸ“ END SECTION (High attention zone):\")\n",
    "    print(f\"   '{end_section[:100]}...'\" if len(end_section) > 100 else f\"   '{end_section}'\")\n",
    "    \n",
    "    return {\n",
    "        'total_tokens': total_tokens,\n",
    "        'sections': {\n",
    "            'start': start_section,\n",
    "            'middle': middle_section,\n",
    "            'end': end_section\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example with a structured prompt\n",
    "structured_prompt = \"\"\"Role: You are a senior data scientist with expertise in machine learning.\n",
    "\n",
    "Context: I'm working on a classification problem with imbalanced data (95% class A, 5% class B).\n",
    "The dataset has 50,000 samples and 20 features. I've tried basic logistic regression with poor results.\n",
    "\n",
    "Task: Recommend a modeling approach that handles class imbalance effectively.\n",
    "\n",
    "Requirements:\n",
    "- Explain why the approach works for imbalanced data\n",
    "- Provide Python code using scikit-learn\n",
    "- Include evaluation metrics appropriate for imbalanced classification\n",
    "\n",
    "Format: Start with a brief explanation, then provide complete, runnable code.\n",
    "\n",
    "Important: Focus on practical solutions I can implement immediately.\"\"\"\n",
    "\n",
    "analyze_prompt_structure(structured_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='4-implementation'></a>\n",
    "## 4. ğŸ› ï¸ Building the Prompt Scorer\n",
    "\n",
    "Now we'll implement a prompt scoring system based on our theoretical foundations.\n",
    "\n",
    "<a id='4.1-methodology'></a>\n",
    "### 4.1 Scoring Methodology\n",
    "\n",
    "Our scorer evaluates prompts on the **six universal dimensions** with weighted scoring:\n",
    "\n",
    "| Dimension | Weight | Rationale |\n",
    "|-----------|--------|----------|\n",
    "| Task Specificity | 25% | Most critical â€“ vague tasks = vague outputs |\n",
    "| Context Depth | 20% | Background enables appropriate responses |\n",
    "| Role Clarity | 15% | Activates relevant knowledge patterns |\n",
    "| Output Format | 15% | Ensures usable results |\n",
    "| Constraints | 15% | Prevents unwanted content |\n",
    "| Style/Tone | 10% | Important but often implicit |\n",
    "\n",
    "**Total: 100 points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Prompt Analyzer Class\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class DimensionScore:\n",
    "    \"\"\"Represents the score for a single dimension.\"\"\"\n",
    "    name: str\n",
    "    score: float\n",
    "    max_score: float\n",
    "    feedback: str\n",
    "    suggestions: List[str]\n",
    "    \n",
    "    @property\n",
    "    def percentage(self) -> float:\n",
    "        return (self.score / self.max_score) * 100 if self.max_score > 0 else 0\n",
    "\n",
    "@dataclass  \n",
    "class PromptAnalysis:\n",
    "    \"\"\"Complete analysis of a prompt.\"\"\"\n",
    "    original_prompt: str\n",
    "    total_score: float\n",
    "    dimension_scores: Dict[str, DimensionScore]\n",
    "    token_count: int\n",
    "    detected_framework: Optional[str]\n",
    "    overall_feedback: str\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'total_score': self.total_score,\n",
    "            'token_count': self.token_count,\n",
    "            'detected_framework': self.detected_framework,\n",
    "            'dimensions': {\n",
    "                name: {\n",
    "                    'score': ds.score,\n",
    "                    'max': ds.max_score,\n",
    "                    'percentage': ds.percentage,\n",
    "                    'feedback': ds.feedback\n",
    "                }\n",
    "                for name, ds in self.dimension_scores.items()\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptAnalyzer:\n",
    "    \"\"\"\n",
    "    Rule-based prompt analyzer that scores prompts on six dimensions.\n",
    "    \n",
    "    This analyzer uses pattern matching and heuristics to evaluate\n",
    "    prompt quality without requiring API calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dimension weights (must sum to 100)\n",
    "    WEIGHTS = {\n",
    "        'task_specificity': 25,\n",
    "        'context_depth': 20,\n",
    "        'role_clarity': 15,\n",
    "        'output_format': 15,\n",
    "        'constraints': 15,\n",
    "        'style_tone': 10\n",
    "    }\n",
    "    \n",
    "    # Pattern indicators for each dimension\n",
    "    ROLE_PATTERNS = [\n",
    "        r'\\b(you are|act as|assume the role|as a|pretend to be|imagine you\\'re)\\b',\n",
    "        r'\\b(expert|specialist|professional|consultant|advisor|coach|mentor)\\b',\n",
    "        r'\\brole\\s*:',\n",
    "        r'\\b(developer|engineer|analyst|scientist|writer|designer)\\b'\n",
    "    ]\n",
    "    \n",
    "    CONTEXT_PATTERNS = [\n",
    "        r'\\b(context|background|situation|scenario)\\s*:',\n",
    "        r'\\b(i\\'m working on|i need|my goal|i have|currently)\\b',\n",
    "        r'\\b(because|since|given that|considering)\\b',\n",
    "        r'\\b(project|task|assignment|problem)\\b'\n",
    "    ]\n",
    "    \n",
    "    TASK_PATTERNS = [\n",
    "        r'\\b(task|objective|goal|please|can you|could you|i need you to)\\b',\n",
    "        r'\\b(write|create|generate|explain|analyze|summarize|list|describe)\\b',\n",
    "        r'\\b(help me|assist|provide|give me)\\b',\n",
    "        r'\\b(step[s]?|instruction[s]?)\\b'\n",
    "    ]\n",
    "    \n",
    "    FORMAT_PATTERNS = [\n",
    "        r'\\b(format|structure|output)\\s*:',\n",
    "        r'\\b(bullet|numbered|list|table|json|markdown|code)\\b',\n",
    "        r'\\b(\\d+\\s*words?|\\d+\\s*sentences?|\\d+\\s*paragraphs?)\\b',\n",
    "        r'\\b(sections?|headers?|organize)\\b'\n",
    "    ]\n",
    "    \n",
    "    CONSTRAINT_PATTERNS = [\n",
    "        r'\\b(do not|don\\'t|avoid|exclude|never|without)\\b',\n",
    "        r'\\b(constraint|limit|restriction|boundary)\\b',\n",
    "        r'\\b(only|must|should not|refrain)\\b',\n",
    "        r'\\b(maximum|minimum|at most|at least)\\b'\n",
    "    ]\n",
    "    \n",
    "    STYLE_PATTERNS = [\n",
    "        r'\\b(tone|style|voice|manner)\\s*:',\n",
    "        r'\\b(formal|informal|casual|professional|friendly|serious)\\b',\n",
    "        r'\\b(simple|technical|beginner|advanced|detailed|brief)\\b',\n",
    "        r'\\b(audience|readers?|users?)\\b'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def analyze(self, prompt: str) -> PromptAnalysis:\n",
    "        \"\"\"\n",
    "        Perform comprehensive analysis of a prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt text to analyze\n",
    "            \n",
    "        Returns:\n",
    "            PromptAnalysis object with scores and feedback\n",
    "        \"\"\"\n",
    "        prompt_lower = prompt.lower()\n",
    "        token_count = len(self.encoding.encode(prompt))\n",
    "        \n",
    "        # Score each dimension\n",
    "        dimension_scores = {\n",
    "            'task_specificity': self._score_task(prompt, prompt_lower),\n",
    "            'context_depth': self._score_context(prompt, prompt_lower),\n",
    "            'role_clarity': self._score_role(prompt, prompt_lower),\n",
    "            'output_format': self._score_format(prompt, prompt_lower),\n",
    "            'constraints': self._score_constraints(prompt, prompt_lower),\n",
    "            'style_tone': self._score_style(prompt, prompt_lower)\n",
    "        }\n",
    "        \n",
    "        # Calculate total score\n",
    "        total_score = sum(ds.score for ds in dimension_scores.values())\n",
    "        \n",
    "        # Detect framework usage\n",
    "        detected_framework = self._detect_framework(prompt_lower)\n",
    "        \n",
    "        # Generate overall feedback\n",
    "        overall_feedback = self._generate_overall_feedback(total_score, dimension_scores)\n",
    "        \n",
    "        return PromptAnalysis(\n",
    "            original_prompt=prompt,\n",
    "            total_score=total_score,\n",
    "            dimension_scores=dimension_scores,\n",
    "            token_count=token_count,\n",
    "            detected_framework=detected_framework,\n",
    "            overall_feedback=overall_feedback\n",
    "        )\n",
    "    \n",
    "    def _count_pattern_matches(self, text: str, patterns: List[str]) -> int:\n",
    "        \"\"\"Count how many patterns match in the text.\"\"\"\n",
    "        return sum(1 for p in patterns if re.search(p, text, re.IGNORECASE))\n",
    "    \n",
    "    def _score_role(self, prompt: str, prompt_lower: str) -> DimensionScore:\n",
    "        \"\"\"Score the role/persona clarity dimension.\"\"\"\n",
    "        max_score = self.WEIGHTS['role_clarity']\n",
    "        matches = self._count_pattern_matches(prompt_lower, self.ROLE_PATTERNS)\n",
    "        \n",
    "        # Score based on matches (0-4 patterns)\n",
    "        if matches >= 3:\n",
    "            score = max_score\n",
    "            feedback = \"Excellent role definition with clear expertise specified.\"\n",
    "            suggestions = []\n",
    "        elif matches >= 2:\n",
    "            score = max_score * 0.75\n",
    "            feedback = \"Good role indication, but could be more specific.\"\n",
    "            suggestions = [\"Specify the exact expertise level (e.g., 'senior', '10 years experience')\"]\n",
    "        elif matches >= 1:\n",
    "            score = max_score * 0.4\n",
    "            feedback = \"Basic role mentioned but lacks specificity.\"\n",
    "            suggestions = [\n",
    "                \"Add 'You are a [specific role]' at the start\",\n",
    "                \"Specify relevant expertise or background\"\n",
    "            ]\n",
    "        else:\n",
    "            score = max_score * 0.1\n",
    "            feedback = \"No role or persona defined.\"\n",
    "            suggestions = [\n",
    "                \"Start with 'You are a [role] with expertise in [domain]'\",\n",
    "                \"Consider what expert would best answer this query\"\n",
    "            ]\n",
    "        \n",
    "        return DimensionScore(\n",
    "            name=\"Role Clarity\",\n",
    "            score=round(score, 1),\n",
    "            max_score=max_score,\n",
    "            feedback=feedback,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "    \n",
    "    def _score_context(self, prompt: str, prompt_lower: str) -> DimensionScore:\n",
    "        \"\"\"Score the context depth dimension.\"\"\"\n",
    "        max_score = self.WEIGHTS['context_depth']\n",
    "        matches = self._count_pattern_matches(prompt_lower, self.CONTEXT_PATTERNS)\n",
    "        \n",
    "        # Also consider prompt length as context indicator\n",
    "        word_count = len(prompt.split())\n",
    "        length_bonus = min(word_count / 50, 1) * 0.3  # Up to 30% bonus for length\n",
    "        \n",
    "        if matches >= 3:\n",
    "            score = max_score * (0.85 + length_bonus)\n",
    "            feedback = \"Rich context provided with clear background information.\"\n",
    "            suggestions = []\n",
    "        elif matches >= 2:\n",
    "            score = max_score * (0.6 + length_bonus)\n",
    "            feedback = \"Good context, but could include more specific details.\"\n",
    "            suggestions = [\"Add specific constraints or requirements\", \"Mention relevant background\"]\n",
    "        elif matches >= 1:\n",
    "            score = max_score * (0.35 + length_bonus * 0.5)\n",
    "            feedback = \"Minimal context provided.\"\n",
    "            suggestions = [\n",
    "                \"Add a 'Context:' section with background\",\n",
    "                \"Explain why you need this and how it will be used\"\n",
    "            ]\n",
    "        else:\n",
    "            score = max_score * 0.1\n",
    "            feedback = \"No context or background information.\"\n",
    "            suggestions = [\n",
    "                \"Describe the situation or problem\",\n",
    "                \"Explain the purpose and intended use\",\n",
    "                \"Provide relevant constraints\"\n",
    "            ]\n",
    "        \n",
    "        return DimensionScore(\n",
    "            name=\"Context Depth\",\n",
    "            score=round(min(score, max_score), 1),\n",
    "            max_score=max_score,\n",
    "            feedback=feedback,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "    \n",
    "    def _score_task(self, prompt: str, prompt_lower: str) -> DimensionScore:\n",
    "        \"\"\"Score the task specificity dimension.\"\"\"\n",
    "        max_score = self.WEIGHTS['task_specificity']\n",
    "        matches = self._count_pattern_matches(prompt_lower, self.TASK_PATTERNS)\n",
    "        \n",
    "        # Check for action verbs (strong task indicators)\n",
    "        action_verbs = ['write', 'create', 'generate', 'explain', 'analyze', \n",
    "                        'summarize', 'compare', 'list', 'describe', 'develop',\n",
    "                        'design', 'build', 'review', 'evaluate', 'recommend']\n",
    "        verb_count = sum(1 for v in action_verbs if v in prompt_lower)\n",
    "        \n",
    "        # Check for specificity indicators\n",
    "        has_numbers = bool(re.search(r'\\d+', prompt))\n",
    "        has_specifics = bool(re.search(r'\\b(specific|exactly|precisely|particular)\\b', prompt_lower))\n",
    "        \n",
    "        base_score = matches / 4  # Normalize to 0-1\n",
    "        verb_bonus = min(verb_count / 3, 1) * 0.2\n",
    "        specificity_bonus = (0.1 if has_numbers else 0) + (0.1 if has_specifics else 0)\n",
    "        \n",
    "        score_pct = min(base_score + verb_bonus + specificity_bonus, 1)\n",
    "        \n",
    "        if score_pct >= 0.8:\n",
    "            feedback = \"Clear, specific task with well-defined objectives.\"\n",
    "            suggestions = []\n",
    "        elif score_pct >= 0.5:\n",
    "            feedback = \"Task is defined but could be more specific.\"\n",
    "            suggestions = [\"Add specific quantities or measurements\", \"Break into sub-tasks if complex\"]\n",
    "        elif score_pct >= 0.25:\n",
    "            feedback = \"Task is vague and could lead to misaligned outputs.\"\n",
    "            suggestions = [\n",
    "                \"Start with a clear action verb (Write, Create, Analyze...)\",\n",
    "                \"Specify exactly what you want\",\n",
    "                \"Add measurable criteria\"\n",
    "            ]\n",
    "        else:\n",
    "            feedback = \"Task is unclear or missing.\"\n",
    "            suggestions = [\n",
    "                \"Clearly state what you want the AI to do\",\n",
    "                \"Use format: '[Action verb] + [specific object] + [criteria]'\",\n",
    "                \"Example: 'Write a 500-word blog post about X that includes Y'\"\n",
    "            ]\n",
    "        \n",
    "        return DimensionScore(\n",
    "            name=\"Task Specificity\",\n",
    "            score=round(score_pct * max_score, 1),\n",
    "            max_score=max_score,\n",
    "            feedback=feedback,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "    \n",
    "    def _score_format(self, prompt: str, prompt_lower: str) -> DimensionScore:\n",
    "        \"\"\"Score the output format specification dimension.\"\"\"\n",
    "        max_score = self.WEIGHTS['output_format']\n",
    "        matches = self._count_pattern_matches(prompt_lower, self.FORMAT_PATTERNS)\n",
    "        \n",
    "        if matches >= 3:\n",
    "            score = max_score\n",
    "            feedback = \"Excellent format specification with clear structure requirements.\"\n",
    "            suggestions = []\n",
    "        elif matches >= 2:\n",
    "            score = max_score * 0.7\n",
    "            feedback = \"Good format indication.\"\n",
    "            suggestions = [\"Consider specifying exact length or structure\"]\n",
    "        elif matches >= 1:\n",
    "            score = max_score * 0.4\n",
    "            feedback = \"Basic format mentioned.\"\n",
    "            suggestions = [\n",
    "                \"Specify desired length (words, paragraphs, sections)\",\n",
    "                \"Indicate preferred structure (bullets, prose, code)\"\n",
    "            ]\n",
    "        else:\n",
    "            score = max_score * 0.15\n",
    "            feedback = \"No output format specified.\"\n",
    "            suggestions = [\n",
    "                \"Add 'Format:' section with structure requirements\",\n",
    "                \"Specify length (e.g., '300-500 words')\",\n",
    "                \"Indicate structure (e.g., 'Use bullet points', 'Include code examples')\"\n",
    "            ]\n",
    "        \n",
    "        return DimensionScore(\n",
    "            name=\"Output Format\",\n",
    "            score=round(score, 1),\n",
    "            max_score=max_score,\n",
    "            feedback=feedback,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "    \n",
    "    def _score_constraints(self, prompt: str, prompt_lower: str) -> DimensionScore:\n",
    "        \"\"\"Score the constraints/boundaries dimension.\"\"\"\n",
    "        max_score = self.WEIGHTS['constraints']\n",
    "        matches = self._count_pattern_matches(prompt_lower, self.CONSTRAINT_PATTERNS)\n",
    "        \n",
    "        if matches >= 3:\n",
    "            score = max_score\n",
    "            feedback = \"Clear constraints and boundaries defined.\"\n",
    "            suggestions = []\n",
    "        elif matches >= 2:\n",
    "            score = max_score * 0.7\n",
    "            feedback = \"Some constraints specified.\"\n",
    "            suggestions = [\"Consider adding what to avoid\"]\n",
    "        elif matches >= 1:\n",
    "            score = max_score * 0.4\n",
    "            feedback = \"Minimal constraints.\"\n",
    "            suggestions = [\n",
    "                \"Add an 'Avoid:' section\",\n",
    "                \"Specify any limits or boundaries\"\n",
    "            ]\n",
    "        else:\n",
    "            score = max_score * 0.2  # Some credit - constraints aren't always needed\n",
    "            feedback = \"No explicit constraints (may be fine for simple queries).\"\n",
    "            suggestions = [\n",
    "                \"Consider adding 'Do not include...' or 'Avoid...' if relevant\",\n",
    "                \"Specify any boundaries or limits\"\n",
    "            ]\n",
    "        \n",
    "        return DimensionScore(\n",
    "            name=\"Constraints\",\n",
    "            score=round(score, 1),\n",
    "            max_score=max_score,\n",
    "            feedback=feedback,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "    \n",
    "    def _score_style(self, prompt: str, prompt_lower: str) -> DimensionScore:\n",
    "        \"\"\"Score the style/tone dimension.\"\"\"\n",
    "        max_score = self.WEIGHTS['style_tone']\n",
    "        matches = self._count_pattern_matches(prompt_lower, self.STYLE_PATTERNS)\n",
    "        \n",
    "        if matches >= 3:\n",
    "            score = max_score\n",
    "            feedback = \"Excellent style and tone specification.\"\n",
    "            suggestions = []\n",
    "        elif matches >= 2:\n",
    "            score = max_score * 0.7\n",
    "            feedback = \"Good style indication.\"\n",
    "            suggestions = [\"Consider specifying audience\"]\n",
    "        elif matches >= 1:\n",
    "            score = max_score * 0.4\n",
    "            feedback = \"Basic style mentioned.\"\n",
    "            suggestions = [\n",
    "                \"Specify tone (professional, casual, technical)\",\n",
    "                \"Indicate target audience\"\n",
    "            ]\n",
    "        else:\n",
    "            score = max_score * 0.2\n",
    "            feedback = \"No style or tone specified.\"\n",
    "            suggestions = [\n",
    "                \"Add 'Tone:' specification\",\n",
    "                \"Consider: formal/informal, technical/simple, brief/detailed\"\n",
    "            ]\n",
    "        \n",
    "        return DimensionScore(\n",
    "            name=\"Style/Tone\",\n",
    "            score=round(score, 1),\n",
    "            max_score=max_score,\n",
    "            feedback=feedback,\n",
    "            suggestions=suggestions\n",
    "        )\n",
    "    \n",
    "    def _detect_framework(self, prompt_lower: str) -> Optional[str]:\n",
    "        \"\"\"Detect if a specific framework is being used.\"\"\"\n",
    "        # Check for explicit framework markers\n",
    "        if all(marker in prompt_lower for marker in ['context:', 'objective:', 'style:', 'tone:']):\n",
    "            return \"CO-STAR\"\n",
    "        elif all(marker in prompt_lower for marker in ['role:', 'instructions:', 'steps:']):\n",
    "            return \"RISEN\"\n",
    "        elif all(marker in prompt_lower for marker in ['capacity:', 'insight:', 'statement:']):\n",
    "            return \"CRISPE\"\n",
    "        elif all(marker in prompt_lower for marker in ['role:', 'action:', 'context:', 'expectation:']):\n",
    "            return \"RACE\"\n",
    "        return None\n",
    "    \n",
    "    def _generate_overall_feedback(self, score: float, dimensions: Dict[str, DimensionScore]) -> str:\n",
    "        \"\"\"Generate overall feedback based on the analysis.\"\"\"\n",
    "        if score >= 85:\n",
    "            return \"ğŸŒŸ Excellent prompt! Well-structured with clear objectives.\"\n",
    "        elif score >= 70:\n",
    "            return \"âœ… Good prompt with room for minor improvements.\"\n",
    "        elif score >= 50:\n",
    "            return \"âš ï¸ Decent prompt, but missing key elements that could improve results.\"\n",
    "        elif score >= 30:\n",
    "            return \"ğŸ”§ Prompt needs significant improvement to get quality outputs.\"\n",
    "        else:\n",
    "            return \"âŒ Prompt is too vague. Consider using a framework like CO-STAR.\"\n",
    "\n",
    "# Test the analyzer\n",
    "print(\"PromptAnalyzer class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test our analyzer with different prompts\n",
    "\n",
    "analyzer = PromptAnalyzer()\n",
    "\n",
    "def display_analysis(analysis: PromptAnalysis):\n",
    "    \"\"\"Pretty print the analysis results.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROMPT ANALYSIS REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show prompt preview\n",
    "    preview = analysis.original_prompt[:100] + \"...\" if len(analysis.original_prompt) > 100 else analysis.original_prompt\n",
    "    print(f\"\\nğŸ“ Prompt: \\\"{preview}\\\"\")\n",
    "    print(f\"ğŸ“Š Tokens: {analysis.token_count}\")\n",
    "    if analysis.detected_framework:\n",
    "        print(f\"ğŸ¯ Detected Framework: {analysis.detected_framework}\")\n",
    "    \n",
    "    # Overall score with visual bar\n",
    "    score = analysis.total_score\n",
    "    bar_length = 30\n",
    "    filled = int(score / 100 * bar_length)\n",
    "    bar = \"â–ˆ\" * filled + \"â–‘\" * (bar_length - filled)\n",
    "    \n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"OVERALL SCORE: {score:.1f}/100\")\n",
    "    print(f\"[{bar}]\")\n",
    "    print(f\"{analysis.overall_feedback}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    \n",
    "    # Dimension breakdown\n",
    "    print(\"\\nDIMENSION BREAKDOWN:\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    \n",
    "    for name, ds in analysis.dimension_scores.items():\n",
    "        dim_bar_len = 20\n",
    "        dim_filled = int(ds.percentage / 100 * dim_bar_len)\n",
    "        dim_bar = \"â–ˆ\" * dim_filled + \"â–‘\" * (dim_bar_len - dim_filled)\n",
    "        \n",
    "        print(f\"\\n{ds.name}:\")\n",
    "        print(f\"  Score: {ds.score}/{ds.max_score} ({ds.percentage:.0f}%)  [{dim_bar}]\")\n",
    "        print(f\"  {ds.feedback}\")\n",
    "        if ds.suggestions:\n",
    "            print(f\"  ğŸ’¡ Suggestions:\")\n",
    "            for suggestion in ds.suggestions:\n",
    "                print(f\"     â€¢ {suggestion}\")\n",
    "\n",
    "# Test with different quality prompts\n",
    "test_prompts = [\n",
    "    # Bad prompt\n",
    "    \"Write about machine learning\",\n",
    "    \n",
    "    # Medium prompt  \n",
    "    \"Explain how neural networks work. Use simple terms and include examples.\",\n",
    "    \n",
    "    # Good prompt\n",
    "    \"\"\"Role: You are a machine learning educator with experience teaching beginners.\n",
    "\n",
    "Context: I'm a software developer with Python experience but no ML background.\n",
    "I need to understand neural networks for a work project.\n",
    "\n",
    "Task: Explain how neural networks work, covering:\n",
    "1. Basic architecture (neurons, layers)\n",
    "2. How training works (forward/backward propagation)\n",
    "3. A simple practical example\n",
    "\n",
    "Format: Use clear sections with headers. Include a simple Python code snippet.\n",
    "Length: 500-700 words.\n",
    "\n",
    "Tone: Technical but accessible. Avoid excessive jargon.\n",
    "\n",
    "Do not: Include complex math or assume prior ML knowledge.\"\"\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n\\n{'ğŸ”´' if i==0 else 'ğŸŸ¡' if i==1 else 'ğŸŸ¢'} TEST {i+1}: {'Bad' if i==0 else 'Medium' if i==1 else 'Good'} Prompt\")\n",
    "    analysis = analyzer.analyze(prompt)\n",
    "    display_analysis(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='4.3-llm-based'></a>\n",
    "### 4.3 LLM-Based Quality Assessment\n",
    "\n",
    "While rule-based analysis catches structural elements, **semantic quality** requires LLM evaluation.\n",
    "\n",
    "We'll use the Gemini API to:\n",
    "1. Evaluate prompt clarity and coherence\n",
    "2. Identify ambiguities a rule-based system might miss\n",
    "3. Generate optimized versions of the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Google's Generative AI library\n",
    "!pip install google-generativeai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "# Configure API - In Colab, use userdata.get() for secure key storage\n",
    "# Go to: Runtime > Manage secrets > Add your GEMINI_API_KEY\n",
    "try:\n",
    "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"âœ… Gemini API configured successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ API key not found. Please add GEMINI_API_KEY to Colab secrets.\")\n",
    "    print(\"   Go to: Runtime > Manage secrets > Add new secret\")\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMPromptEvaluator:\n",
    "    \"\"\"\n",
    "    Uses LLM (Gemini) to perform semantic evaluation of prompts.\n",
    "    \n",
    "    This complements the rule-based analyzer by catching:\n",
    "    - Logical inconsistencies\n",
    "    - Ambiguous phrasing\n",
    "    - Missing context that humans would notice\n",
    "    - Quality of examples provided\n",
    "    \"\"\"\n",
    "    \n",
    "    EVALUATION_PROMPT = \"\"\"You are an expert prompt engineer evaluating the quality of prompts for AI systems.\n",
    "\n",
    "Analyze the following prompt and provide:\n",
    "1. A quality score from 0-100\n",
    "2. Specific issues identified (list each)\n",
    "3. What's working well (list each)\n",
    "4. An optimized version of the prompt\n",
    "\n",
    "PROMPT TO EVALUATE:\n",
    "```\n",
    "{prompt}\n",
    "```\n",
    "\n",
    "Respond in this exact JSON format:\n",
    "{{\n",
    "    \"score\": <number 0-100>,\n",
    "    \"issues\": [\"issue1\", \"issue2\", ...],\n",
    "    \"strengths\": [\"strength1\", \"strength2\", ...],\n",
    "    \"optimized_prompt\": \"<the improved prompt>\",\n",
    "    \"explanation\": \"<brief explanation of changes made>\"\n",
    "}}\n",
    "\n",
    "Important: Return ONLY valid JSON, no additional text.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini-1.5-flash\"):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Gemini model to use (flash is faster, pro is more capable)\n",
    "        \"\"\"\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def evaluate(self, prompt: str) -> dict:\n",
    "        \"\"\"\n",
    "        Evaluate a prompt using the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with score, issues, strengths, and optimized version\n",
    "        \"\"\"\n",
    "        try:\n",
    "            evaluation_request = self.EVALUATION_PROMPT.format(prompt=prompt)\n",
    "            response = self.model.generate_content(evaluation_request)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            response_text = response.text.strip()\n",
    "            # Remove markdown code blocks if present\n",
    "            if response_text.startswith(\"```\"):\n",
    "                response_text = response_text.split(\"```\")[1]\n",
    "                if response_text.startswith(\"json\"):\n",
    "                    response_text = response_text[4:]\n",
    "            \n",
    "            result = json.loads(response_text)\n",
    "            result['model_used'] = self.model_name\n",
    "            result['success'] = True\n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'Failed to parse LLM response: {e}',\n",
    "                'raw_response': response.text if 'response' in dir() else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def optimize(self, prompt: str, framework: str = \"CO-STAR\") -> dict:\n",
    "        \"\"\"\n",
    "        Optimize a prompt using a specific framework.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Original prompt\n",
    "            framework: Framework to apply (CO-STAR, CRISPE, RISEN, RACE)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with optimized prompt and explanation\n",
    "        \"\"\"\n",
    "        framework_instructions = {\n",
    "            \"CO-STAR\": \"\"\"Apply the CO-STAR framework:\n",
    "                - Context: Background information\n",
    "                - Objective: Specific goal\n",
    "                - Style: Writing style\n",
    "                - Tone: Emotional quality\n",
    "                - Audience: Who will read it\n",
    "                - Response: Format requirements\"\"\",\n",
    "            \"CRISPE\": \"\"\"Apply the CRISPE framework:\n",
    "                - Capacity/Role: AI expertise\n",
    "                - Insight: Background context\n",
    "                - Statement: Specific task\n",
    "                - Personality: Voice/style\n",
    "                - Experiment: Request variations\n",
    "                - Exclusions: What to avoid\"\"\",\n",
    "            \"RISEN\": \"\"\"Apply the RISEN framework:\n",
    "                - Role: AI expertise\n",
    "                - Instructions: Detailed task\n",
    "                - Steps: Break into stages\n",
    "                - End goal: Success criteria\n",
    "                - Narrowing: Constraints\"\"\",\n",
    "            \"RACE\": \"\"\"Apply the RACE framework:\n",
    "                - Role: Who is the AI?\n",
    "                - Action: What to do?\n",
    "                - Context: Background\n",
    "                - Expectation: Output format\"\"\"\n",
    "        }\n",
    "        \n",
    "        optimization_prompt = f\"\"\"You are an expert prompt engineer.\n",
    "\n",
    "Take this prompt and rewrite it using the {framework} framework.\n",
    "\n",
    "{framework_instructions.get(framework, framework_instructions['CO-STAR'])}\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "```\n",
    "{prompt}\n",
    "```\n",
    "\n",
    "Respond with JSON:\n",
    "{{\n",
    "    \"framework_used\": \"{framework}\",\n",
    "    \"optimized_prompt\": \"<the rewritten prompt>\",\n",
    "    \"changes_made\": [\"change1\", \"change2\", ...],\n",
    "    \"expected_improvement\": \"<why this is better>\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.generate_content(optimization_prompt)\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            if response_text.startswith(\"```\"):\n",
    "                response_text = response_text.split(\"```\")[1]\n",
    "                if response_text.startswith(\"json\"):\n",
    "                    response_text = response_text[4:]\n",
    "            \n",
    "            result = json.loads(response_text)\n",
    "            result['success'] = True\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "print(\"LLMPromptEvaluator class created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM evaluator (requires API key)\n",
    "try:\n",
    "    llm_evaluator = LLMPromptEvaluator()\n",
    "    \n",
    "    test_prompt = \"Write about machine learning\"\n",
    "    \n",
    "    print(\"Testing LLM Evaluation...\")\n",
    "    print(f\"Original prompt: '{test_prompt}'\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluation = llm_evaluator.evaluate(test_prompt)\n",
    "    \n",
    "    if evaluation.get('success'):\n",
    "        print(f\"\\nğŸ“Š LLM Quality Score: {evaluation['score']}/100\")\n",
    "        \n",
    "        print(f\"\\nâŒ Issues Identified:\")\n",
    "        for issue in evaluation.get('issues', []):\n",
    "            print(f\"   â€¢ {issue}\")\n",
    "        \n",
    "        print(f\"\\nâœ… Strengths:\")\n",
    "        for strength in evaluation.get('strengths', []):\n",
    "            print(f\"   â€¢ {strength}\")\n",
    "        \n",
    "        print(f\"\\nğŸ”§ Optimized Prompt:\")\n",
    "        print(f\"   {evaluation.get('optimized_prompt', 'N/A')}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ Explanation:\")\n",
    "        print(f\"   {evaluation.get('explanation', 'N/A')}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Evaluation failed: {evaluation.get('error')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not test LLM evaluator: {e}\")\n",
    "    print(\"   Make sure to add your GEMINI_API_KEY to Colab secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.4-combined'></a>\n",
    "### 4.4 Combined Scoring System\n",
    "\n",
    "Now let's combine both approaches for a comprehensive evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCraft:\n",
    "    \"\"\"\n",
    "    Complete prompt analysis and optimization system.\n",
    "    \n",
    "    Combines:\n",
    "    - Rule-based structural analysis\n",
    "    - LLM-based semantic evaluation\n",
    "    - Multi-framework optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_llm: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize PromptCraft.\n",
    "        \n",
    "        Args:\n",
    "            use_llm: Whether to use LLM for evaluation (requires API key)\n",
    "        \"\"\"\n",
    "        self.rule_analyzer = PromptAnalyzer()\n",
    "        self.use_llm = use_llm\n",
    "        \n",
    "        if use_llm:\n",
    "            try:\n",
    "                self.llm_evaluator = LLMPromptEvaluator()\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ LLM evaluator unavailable: {e}\")\n",
    "                self.use_llm = False\n",
    "    \n",
    "    def analyze(self, prompt: str) -> dict:\n",
    "        \"\"\"\n",
    "        Perform complete analysis of a prompt.\n",
    "        \n",
    "        Returns combined scores from rule-based and LLM analysis.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'original_prompt': prompt,\n",
    "            'rule_based': None,\n",
    "            'llm_based': None,\n",
    "            'combined_score': None,\n",
    "            'grade': None\n",
    "        }\n",
    "        \n",
    "        # Rule-based analysis\n",
    "        rule_analysis = self.rule_analyzer.analyze(prompt)\n",
    "        results['rule_based'] = {\n",
    "            'score': rule_analysis.total_score,\n",
    "            'token_count': rule_analysis.token_count,\n",
    "            'dimensions': rule_analysis.to_dict()['dimensions'],\n",
    "            'feedback': rule_analysis.overall_feedback\n",
    "        }\n",
    "        \n",
    "        # LLM-based analysis (if available)\n",
    "        if self.use_llm:\n",
    "            llm_result = self.llm_evaluator.evaluate(prompt)\n",
    "            if llm_result.get('success'):\n",
    "                results['llm_based'] = {\n",
    "                    'score': llm_result['score'],\n",
    "                    'issues': llm_result.get('issues', []),\n",
    "                    'strengths': llm_result.get('strengths', []),\n",
    "                    'optimized_prompt': llm_result.get('optimized_prompt'),\n",
    "                    'explanation': llm_result.get('explanation')\n",
    "                }\n",
    "        \n",
    "        # Calculate combined score\n",
    "        if results['llm_based']:\n",
    "            # Weight: 60% rule-based, 40% LLM\n",
    "            combined = (results['rule_based']['score'] * 0.6 + \n",
    "                       results['llm_based']['score'] * 0.4)\n",
    "        else:\n",
    "            combined = results['rule_based']['score']\n",
    "        \n",
    "        results['combined_score'] = round(combined, 1)\n",
    "        results['grade'] = self._score_to_grade(combined)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def optimize(self, prompt: str, framework: str = \"CO-STAR\") -> dict:\n",
    "        \"\"\"\n",
    "        Optimize a prompt using the specified framework.\n",
    "        \"\"\"\n",
    "        if not self.use_llm:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'LLM required for optimization'\n",
    "            }\n",
    "        \n",
    "        return self.llm_evaluator.optimize(prompt, framework)\n",
    "    \n",
    "    def _score_to_grade(self, score: float) -> str:\n",
    "        \"\"\"Convert numeric score to letter grade.\"\"\"\n",
    "        if score >= 90:\n",
    "            return \"A+\"\n",
    "        elif score >= 85:\n",
    "            return \"A\"\n",
    "        elif score >= 80:\n",
    "            return \"A-\"\n",
    "        elif score >= 75:\n",
    "            return \"B+\"\n",
    "        elif score >= 70:\n",
    "            return \"B\"\n",
    "        elif score >= 65:\n",
    "            return \"B-\"\n",
    "        elif score >= 60:\n",
    "            return \"C+\"\n",
    "        elif score >= 55:\n",
    "            return \"C\"\n",
    "        elif score >= 50:\n",
    "            return \"C-\"\n",
    "        elif score >= 40:\n",
    "            return \"D\"\n",
    "        else:\n",
    "            return \"F\"\n",
    "\n",
    "print(\"PromptCraft complete system created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test of PromptCraft\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def visualize_promptcraft_analysis(results: dict):\n",
    "    \"\"\"Create a visual dashboard of the prompt analysis.\"\"\"\n",
    "    \n",
    "    # Create gauge chart for overall score\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        specs=[[{\"type\": \"indicator\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"table\", \"colspan\": 2}, None]],\n",
    "        subplot_titles=(\"Overall Score\", \"Dimension Breakdown\", \"Detailed Feedback\")\n",
    "    )\n",
    "    \n",
    "    # Gauge for overall score\n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=results['combined_score'],\n",
    "            title={'text': f\"Grade: {results['grade']}\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [0, 100]},\n",
    "                'bar': {'color': \"darkblue\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 40], 'color': \"#ff6b6b\"},\n",
    "                    {'range': [40, 60], 'color': \"#ffd93d\"},\n",
    "                    {'range': [60, 80], 'color': \"#6bcb77\"},\n",
    "                    {'range': [80, 100], 'color': \"#4d96ff\"}\n",
    "                ],\n",
    "                'threshold': {\n",
    "                    'line': {'color': \"red\", 'width': 4},\n",
    "                    'thickness': 0.75,\n",
    "                    'value': 70\n",
    "                }\n",
    "            }\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Bar chart for dimensions\n",
    "    dimensions = results['rule_based']['dimensions']\n",
    "    dim_names = list(dimensions.keys())\n",
    "    dim_scores = [dimensions[d]['score'] for d in dim_names]\n",
    "    dim_max = [dimensions[d]['max'] for d in dim_names]\n",
    "    dim_pct = [dimensions[d]['percentage'] for d in dim_names]\n",
    "    \n",
    "    # Color based on percentage\n",
    "    colors = ['#ff6b6b' if p < 40 else '#ffd93d' if p < 70 else '#6bcb77' for p in dim_pct]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[d.replace('_', ' ').title() for d in dim_names],\n",
    "            y=dim_scores,\n",
    "            marker_color=colors,\n",
    "            text=[f\"{p:.0f}%\" for p in dim_pct],\n",
    "            textposition='auto',\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add max score line\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[d.replace('_', ' ').title() for d in dim_names],\n",
    "            y=dim_max,\n",
    "            mode='markers',\n",
    "            marker=dict(symbol='line-ew', size=20, color='black'),\n",
    "            name='Max Score'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Table with feedback\n",
    "    feedback_data = []\n",
    "    for d in dim_names:\n",
    "        feedback_data.append([\n",
    "            d.replace('_', ' ').title(),\n",
    "            f\"{dimensions[d]['score']}/{dimensions[d]['max']}\",\n",
    "            dimensions[d]['feedback']\n",
    "        ])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=['Dimension', 'Score', 'Feedback'],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=list(zip(*feedback_data)),\n",
    "                fill_color='lavender',\n",
    "                align='left'\n",
    "            )\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        showlegend=False,\n",
    "        title_text=\"PromptCraft Analysis Dashboard\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Run analysis\n",
    "try:\n",
    "    promptcraft = PromptCraft(use_llm=True)\n",
    "except:\n",
    "    promptcraft = PromptCraft(use_llm=False)\n",
    "    print(\"Running without LLM (API key not configured)\")\n",
    "\n",
    "# Test with a medium-quality prompt\n",
    "test_prompt = \"\"\"I need help writing a cover letter for a data science job. \n",
    "I have 3 years of experience with Python and machine learning.\n",
    "Make it professional and highlight my skills.\"\"\"\n",
    "\n",
    "print(f\"Analyzing prompt: '{test_prompt[:50]}...'\")\n",
    "results = promptcraft.analyze(test_prompt)\n",
    "\n",
    "visualize_promptcraft_analysis(results)\n",
    "\n",
    "# Show LLM insights if available\n",
    "if results.get('llm_based'):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LLM INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nIssues: {results['llm_based']['issues']}\")\n",
    "    print(f\"\\nStrengths: {results['llm_based']['strengths']}\")\n",
    "    print(f\"\\nOptimized version available: {'Yes' if results['llm_based'].get('optimized_prompt') else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='5-optimization'></a>\n",
    "## 5. ğŸš€ Prompt Optimization Engine\n",
    "\n",
    "Now let's build the optimization component that improves prompts using different frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_optimization(original_prompt: str):\n",
    "    \"\"\"\n",
    "    Demonstrate how the same prompt gets optimized using different frameworks.\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PROMPT OPTIMIZATION DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nğŸ“ Original Prompt:\\n'{original_prompt}'\")\n",
    "    \n",
    "    # Analyze original\n",
    "    original_analysis = promptcraft.analyze(original_prompt)\n",
    "    print(f\"\\nğŸ“Š Original Score: {original_analysis['combined_score']}/100 ({original_analysis['grade']})\")\n",
    "    \n",
    "    # Optimize with each framework\n",
    "    frameworks = [\"CO-STAR\", \"CRISPE\", \"RISEN\", \"RACE\"]\n",
    "    \n",
    "    optimized_results = []\n",
    "    \n",
    "    for framework in frameworks:\n",
    "        print(f\"\\n{'â”€'*70}\")\n",
    "        print(f\"ğŸ”§ Optimizing with {framework} framework...\")\n",
    "        \n",
    "        result = promptcraft.optimize(original_prompt, framework)\n",
    "        \n",
    "        if result.get('success'):\n",
    "            optimized_prompt = result.get('optimized_prompt', '')\n",
    "            \n",
    "            # Analyze optimized version\n",
    "            optimized_analysis = promptcraft.analyze(optimized_prompt)\n",
    "            \n",
    "            print(f\"\\nâœ… Optimized Prompt ({framework}):\")\n",
    "            print(f\"{optimized_prompt[:500]}...\" if len(optimized_prompt) > 500 else optimized_prompt)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š New Score: {optimized_analysis['combined_score']}/100 ({optimized_analysis['grade']})\")\n",
    "            improvement = optimized_analysis['combined_score'] - original_analysis['combined_score']\n",
    "            print(f\"ğŸ“ˆ Improvement: {'+' if improvement > 0 else ''}{improvement:.1f} points\")\n",
    "            \n",
    "            optimized_results.append({\n",
    "                'framework': framework,\n",
    "                'prompt': optimized_prompt,\n",
    "                'score': optimized_analysis['combined_score'],\n",
    "                'improvement': improvement\n",
    "            })\n",
    "        else:\n",
    "            print(f\"âŒ Optimization failed: {result.get('error')}\")\n",
    "    \n",
    "    # Summary\n",
    "    if optimized_results:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"OPTIMIZATION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        best = max(optimized_results, key=lambda x: x['score'])\n",
    "        print(f\"\\nğŸ† Best Framework: {best['framework']} (Score: {best['score']})\")\n",
    "        print(f\"\\nğŸ“Š All Results:\")\n",
    "        for r in sorted(optimized_results, key=lambda x: x['score'], reverse=True):\n",
    "            print(f\"   {r['framework']}: {r['score']}/100 ({'+' if r['improvement'] > 0 else ''}{r['improvement']:.1f})\")\n",
    "    \n",
    "    return optimized_results\n",
    "\n",
    "# Run demonstration (requires LLM)\n",
    "if promptcraft.use_llm:\n",
    "    test_prompt = \"Help me write an email to my boss about a raise\"\n",
    "    results = demonstrate_optimization(test_prompt)\n",
    "else:\n",
    "    print(\"âš ï¸ LLM not available. Add GEMINI_API_KEY to run optimization demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='6-comparison'></a>\n",
    "## 6. ğŸ”„ Multi-Model Comparison (Bonus)\n",
    "\n",
    "Let's compare how different models respond to the same prompt before and after optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section can be extended with Claude API comparison\n",
    "# For now, we demonstrate with Gemini only\n",
    "\n",
    "def compare_outputs(prompt: str, optimized_prompt: str):\n",
    "    \"\"\"\n",
    "    Compare LLM outputs for original vs optimized prompts.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"OUTPUT COMPARISON: Original vs Optimized Prompt\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Generate with original prompt\n",
    "    print(\"\\nğŸ“ Original Prompt Response:\")\n",
    "    print(\"â”€\"*50)\n",
    "    original_response = model.generate_content(prompt)\n",
    "    print(original_response.text[:500] + \"...\" if len(original_response.text) > 500 else original_response.text)\n",
    "    \n",
    "    # Generate with optimized prompt\n",
    "    print(\"\\n\\nğŸš€ Optimized Prompt Response:\")\n",
    "    print(\"â”€\"*50)\n",
    "    optimized_response = model.generate_content(optimized_prompt)\n",
    "    print(optimized_response.text[:500] + \"...\" if len(optimized_response.text) > 500 else optimized_response.text)\n",
    "    \n",
    "    # Metrics comparison\n",
    "    print(\"\\n\\nğŸ“Š Response Metrics:\")\n",
    "    print(\"â”€\"*50)\n",
    "    print(f\"Original response length: {len(original_response.text)} chars\")\n",
    "    print(f\"Optimized response length: {len(optimized_response.text)} chars\")\n",
    "\n",
    "# Demo (requires API)\n",
    "if promptcraft.use_llm:\n",
    "    original = \"Write about data science careers\"\n",
    "    optimized = \"\"\"Role: You are a career counselor specializing in tech and data careers with 10 years of experience.\n",
    "\n",
    "Context: I'm a recent graduate with a statistics degree considering data science as a career path.\n",
    "\n",
    "Task: Explain the data science career landscape, including:\n",
    "1. Common job titles and their differences (Data Analyst vs Data Scientist vs ML Engineer)\n",
    "2. Required skills for each level\n",
    "3. Typical career progression\n",
    "4. Salary expectations\n",
    "\n",
    "Format: Use clear sections. Include a simple career progression diagram in text.\n",
    "Length: 400-500 words.\n",
    "Tone: Encouraging but realistic. Focus on actionable insights.\n",
    "\n",
    "Avoid: Overly technical jargon or discouraging language.\"\"\"\n",
    "    \n",
    "    compare_outputs(original, optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='7-conclusion'></a>\n",
    "## 7. ğŸ“Œ Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Prompt engineering is systematic**: Frameworks like CO-STAR, CRISPE, RISEN, and RACE provide structured approaches that consistently improve results.\n",
    "\n",
    "2. **Six universal dimensions**: Role, Context, Task, Format, Constraints, and Style/Tone appear across all frameworks â€“ optimizing these dimensions improves any prompt.\n",
    "\n",
    "3. **Technical understanding helps**: Knowing how LLMs process prompts (tokenization, attention, context windows) explains *why* certain techniques work.\n",
    "\n",
    "4. **Hybrid evaluation works best**: Combining rule-based structural analysis with LLM semantic evaluation provides comprehensive prompt assessment.\n",
    "\n",
    "5. **Small improvements compound**: A well-structured prompt with 100 tokens dramatically outperforms a vague prompt with 20 tokens.\n",
    "\n",
    "### Impact on Generative AI Applications\n",
    "\n",
    "| Domain | Application of Prompt Engineering |\n",
    "|--------|----------------------------------|\n",
    "| **NLP** | Better chatbots, more accurate summarization, controlled text generation |\n",
    "| **Computer Vision** | More precise image generation prompts, better vision-language models |\n",
    "| **Creative Industries** | Consistent brand voice, structured creative briefs, quality content at scale |\n",
    "| **Enterprise** | Reliable AI assistants, auditable prompt templates, reduced hallucination |\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "- **Automatic prompt optimization**: Systems that iteratively improve prompts based on output quality\n",
    "- **Domain-specific frameworks**: Specialized prompt structures for coding, legal, medical, etc.\n",
    "- **Prompt testing suites**: Standardized evaluation of prompt effectiveness across models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8-references'></a>\n",
    "## 8. ğŸ“š References\n",
    "\n",
    "### Academic Papers\n",
    "\n",
    "1. Wei, J., et al. (2022). \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.\" *NeurIPS 2022*.\n",
    "\n",
    "2. Liu, P., et al. (2023). \"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.\" *ACM Computing Surveys*.\n",
    "\n",
    "3. Zhou, Y., et al. (2022). \"Large Language Models Are Human-Level Prompt Engineers.\" *arXiv:2211.01910*.\n",
    "\n",
    "4. Reynolds, L., & McDonell, K. (2021). \"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm.\" *CHI EA '21*.\n",
    "\n",
    "### Industry Resources\n",
    "\n",
    "5. Anthropic. (2024). \"Prompt Engineering Guide.\" https://docs.anthropic.com/claude/docs/prompt-engineering\n",
    "\n",
    "6. OpenAI. (2024). \"Best Practices for Prompt Engineering.\" https://platform.openai.com/docs/guides/prompt-engineering\n",
    "\n",
    "7. Google. (2024). \"Prompt Design Strategies.\" https://ai.google.dev/docs/prompt_best_practices\n",
    "\n",
    "8. Singapore GovTech. (2024). \"CO-STAR Framework for Prompt Engineering.\"\n",
    "\n",
    "### Technical Documentation\n",
    "\n",
    "9. tiktoken library: https://github.com/openai/tiktoken\n",
    "\n",
    "10. Google Generative AI Python SDK: https://github.com/google/generative-ai-python\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9-license'></a>\n",
    "## 9. ğŸ“„ License\n",
    "\n",
    "This notebook is released under the **MIT License**.\n",
    "\n",
    "```\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2024 [Your Name]\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Created with AI assistance (Claude by Anthropic) as part of learning AI-assisted development practices.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
